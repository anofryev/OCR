{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc4ba4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def plot_gray(image):\n",
    "    %matplotlib qt\n",
    "    plt.figure(figsize=(20,15))\n",
    "    return plt.imshow(image, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1e0dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_path = './samples/василий/'\n",
    "image_names = os.listdir(samples_path)\n",
    "\n",
    "classes_list = ['василий', 'не_василий']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ca573b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "for i, class_ in enumerate(classes_list):\n",
    "    image_names = os. listdir('./samples/' + class_)\n",
    "    for image_name in images_names:\n",
    "        image_path = './samples/' + class_ + '/'+ image_name\n",
    "        image_class = i\n",
    "        class_name = class_\n",
    "        X_list.append((image_class, class_name, image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2978b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_path = './samples/василий/'\n",
    "image_names = os.listdir(samples_path)\n",
    "\n",
    "classes_list = ['василий', 'не_василий']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8beaf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = []\n",
    "for image_name in image_names:\n",
    "    img = cv2.imread(samples_path + '/' + image_name)\n",
    "    \n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    for d in des:\n",
    "        dico.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8a24373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4978"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cc025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95a5c4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method: k-means++\n",
      "Inertia for init 1/3: 30053998.451577\n",
      "Init 2/3 with method: k-means++\n",
      "Inertia for init 2/3: 30366549.553077\n",
      "Init 3/3 with method: k-means++\n",
      "Inertia for init 3/3: 30398714.813939\n",
      "Minibatch iteration 1/2700: mean batch inertia: 73647.517341, ewa inertia: 73647.517341 \n",
      "Minibatch iteration 2/2700: mean batch inertia: 75321.777802, ewa inertia: 73772.607699 \n",
      "Minibatch iteration 3/2700: mean batch inertia: 74020.972362, ewa inertia: 73791.163966 \n",
      "Minibatch iteration 4/2700: mean batch inertia: 68158.914109, ewa inertia: 73370.357188 \n",
      "Minibatch iteration 5/2700: mean batch inertia: 69151.255191, ewa inertia: 73055.132054 \n",
      "Minibatch iteration 6/2700: mean batch inertia: 73322.911089, ewa inertia: 73075.138843 \n",
      "Minibatch iteration 7/2700: mean batch inertia: 70041.623591, ewa inertia: 72848.493397 \n",
      "Minibatch iteration 8/2700: mean batch inertia: 71091.856612, ewa inertia: 72717.248391 \n",
      "Minibatch iteration 9/2700: mean batch inertia: 73655.044641, ewa inertia: 72787.314711 \n",
      "[MiniBatchKMeans] Reassigning 2 cluster centers.\n",
      "Minibatch iteration 10/2700: mean batch inertia: 68806.508843, ewa inertia: 72489.893585 \n",
      "[MiniBatchKMeans] Reassigning 4 cluster centers.\n",
      "Minibatch iteration 11/2700: mean batch inertia: 69198.266666, ewa inertia: 72243.963637 \n",
      "Minibatch iteration 12/2700: mean batch inertia: 61777.264554, ewa inertia: 71461.956796 \n",
      "Minibatch iteration 13/2700: mean batch inertia: 70987.900417, ewa inertia: 71426.538244 \n",
      "Minibatch iteration 14/2700: mean batch inertia: 72069.513650, ewa inertia: 71474.577378 \n",
      "Minibatch iteration 15/2700: mean batch inertia: 68502.266448, ewa inertia: 71252.504740 \n",
      "Minibatch iteration 16/2700: mean batch inertia: 69217.561055, ewa inertia: 71100.466369 \n",
      "Minibatch iteration 17/2700: mean batch inertia: 65876.526109, ewa inertia: 70710.165952 \n",
      "Minibatch iteration 18/2700: mean batch inertia: 65915.268278, ewa inertia: 70351.920936 \n",
      "Minibatch iteration 19/2700: mean batch inertia: 64232.899083, ewa inertia: 69894.745573 \n",
      "Minibatch iteration 20/2700: mean batch inertia: 67876.286783, ewa inertia: 69743.938851 \n",
      "Minibatch iteration 21/2700: mean batch inertia: 66683.343144, ewa inertia: 69515.270122 \n",
      "Minibatch iteration 22/2700: mean batch inertia: 67027.142402, ewa inertia: 69329.372650 \n",
      "Minibatch iteration 23/2700: mean batch inertia: 67850.573114, ewa inertia: 69218.885920 \n",
      "[MiniBatchKMeans] Reassigning 2 cluster centers.\n",
      "Minibatch iteration 24/2700: mean batch inertia: 68769.437321, ewa inertia: 69185.305908 \n",
      "Minibatch iteration 25/2700: mean batch inertia: 67978.479211, ewa inertia: 69095.139302 \n",
      "Minibatch iteration 26/2700: mean batch inertia: 67826.312367, ewa inertia: 69000.340423 \n",
      "Minibatch iteration 27/2700: mean batch inertia: 66585.786003, ewa inertia: 68819.939892 \n",
      "Minibatch iteration 28/2700: mean batch inertia: 66196.892063, ewa inertia: 68623.962027 \n",
      "Minibatch iteration 29/2700: mean batch inertia: 68043.976049, ewa inertia: 68580.629071 \n",
      "Minibatch iteration 30/2700: mean batch inertia: 69751.586836, ewa inertia: 68668.115773 \n",
      "Minibatch iteration 31/2700: mean batch inertia: 62819.301291, ewa inertia: 68231.128630 \n",
      "Minibatch iteration 32/2700: mean batch inertia: 65096.675782, ewa inertia: 67996.941753 \n",
      "Minibatch iteration 33/2700: mean batch inertia: 62370.210247, ewa inertia: 67576.547272 \n",
      "Minibatch iteration 34/2700: mean batch inertia: 64081.110417, ewa inertia: 67315.389909 \n",
      "Minibatch iteration 35/2700: mean batch inertia: 64249.354101, ewa inertia: 67086.314729 \n",
      "Minibatch iteration 36/2700: mean batch inertia: 64900.510218, ewa inertia: 66923.004973 \n",
      "Minibatch iteration 37/2700: mean batch inertia: 64330.049393, ewa inertia: 66729.275413 \n",
      "Minibatch iteration 38/2700: mean batch inertia: 67830.328094, ewa inertia: 66811.539241 \n",
      "Minibatch iteration 39/2700: mean batch inertia: 64064.488364, ewa inertia: 66606.296637 \n",
      "Minibatch iteration 40/2700: mean batch inertia: 67718.361893, ewa inertia: 66689.383256 \n",
      "Minibatch iteration 41/2700: mean batch inertia: 66122.496787, ewa inertia: 66647.029014 \n",
      "Minibatch iteration 42/2700: mean batch inertia: 65883.627955, ewa inertia: 66589.992422 \n",
      "Minibatch iteration 43/2700: mean batch inertia: 67378.895534, ewa inertia: 66648.934370 \n",
      "Minibatch iteration 44/2700: mean batch inertia: 67682.151257, ewa inertia: 66726.129927 \n",
      "Minibatch iteration 45/2700: mean batch inertia: 68013.719663, ewa inertia: 66822.330647 \n",
      "Minibatch iteration 46/2700: mean batch inertia: 69372.956150, ewa inertia: 67012.897565 \n",
      "Minibatch iteration 47/2700: mean batch inertia: 63120.239886, ewa inertia: 66722.062326 \n",
      "Minibatch iteration 48/2700: mean batch inertia: 65211.969659, ewa inertia: 66609.237568 \n",
      "Minibatch iteration 49/2700: mean batch inertia: 64836.391480, ewa inertia: 66476.781503 \n",
      "Minibatch iteration 50/2700: mean batch inertia: 67515.601428, ewa inertia: 66554.395685 \n",
      "Minibatch iteration 51/2700: mean batch inertia: 64814.930581, ewa inertia: 66424.433641 \n",
      "Minibatch iteration 52/2700: mean batch inertia: 65582.871723, ewa inertia: 66361.557354 \n",
      "Minibatch iteration 53/2700: mean batch inertia: 68124.542423, ewa inertia: 66493.276664 \n",
      "Minibatch iteration 54/2700: mean batch inertia: 69944.096901, ewa inertia: 66751.100550 \n",
      "Minibatch iteration 55/2700: mean batch inertia: 62570.081648, ewa inertia: 66438.720748 \n",
      "Minibatch iteration 56/2700: mean batch inertia: 62850.907559, ewa inertia: 66170.661599 \n",
      "Minibatch iteration 57/2700: mean batch inertia: 64003.107078, ewa inertia: 66008.715368 \n",
      "Minibatch iteration 58/2700: mean batch inertia: 67388.021193, ewa inertia: 66111.768545 \n",
      "Minibatch iteration 59/2700: mean batch inertia: 65101.390911, ewa inertia: 66036.279395 \n",
      "Minibatch iteration 60/2700: mean batch inertia: 63295.686793, ewa inertia: 65831.519313 \n",
      "Minibatch iteration 61/2700: mean batch inertia: 64360.522907, ewa inertia: 65721.615584 \n",
      "Minibatch iteration 62/2700: mean batch inertia: 65278.682230, ewa inertia: 65688.522351 \n",
      "Minibatch iteration 63/2700: mean batch inertia: 64866.474446, ewa inertia: 65627.104030 \n",
      "Minibatch iteration 64/2700: mean batch inertia: 67265.030740, ewa inertia: 65749.479755 \n",
      "Minibatch iteration 65/2700: mean batch inertia: 65385.327904, ewa inertia: 65722.272587 \n",
      "Minibatch iteration 66/2700: mean batch inertia: 67873.983645, ewa inertia: 65883.035093 \n",
      "Minibatch iteration 67/2700: mean batch inertia: 66984.303848, ewa inertia: 65965.315064 \n",
      "Minibatch iteration 68/2700: mean batch inertia: 70128.992672, ewa inertia: 66276.399231 \n",
      "Minibatch iteration 69/2700: mean batch inertia: 64648.548405, ewa inertia: 66154.776314 \n",
      "Minibatch iteration 70/2700: mean batch inertia: 64545.461723, ewa inertia: 66034.538309 \n",
      "Minibatch iteration 71/2700: mean batch inertia: 64794.042633, ewa inertia: 65941.856165 \n",
      "Minibatch iteration 72/2700: mean batch inertia: 67243.226024, ewa inertia: 66039.086450 \n",
      "Minibatch iteration 73/2700: mean batch inertia: 66781.419252, ewa inertia: 66094.548953 \n",
      "Converged (lack of improvement in inertia) at iteration 73/2700\n",
      "Computing label assignment and total inertia\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "\n",
    "batch_size = np.size(image_names) * 3\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, verbose=1).fit(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a2a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd1b5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans.verbose = False\n",
    "\n",
    "histo_list = []\n",
    "\n",
    "for tup in X_list:\n",
    "    img = cv2.imread(tup[2],0)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "\n",
    "    histo = np.zeros(k)\n",
    "    nkp = np.size(kp)\n",
    "\n",
    "    for d in des:\n",
    "        idx = kmeans.predict([d])\n",
    "        histo[idx] += 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n",
    "\n",
    "    histo_list.append(histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b84db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f606244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70915468\n",
      "Iteration 2, loss = 0.70725093\n",
      "Iteration 3, loss = 0.70541800\n",
      "Iteration 4, loss = 0.70364771\n",
      "Iteration 5, loss = 0.70193940\n",
      "Iteration 6, loss = 0.70028842\n",
      "Iteration 7, loss = 0.69869870\n",
      "Iteration 8, loss = 0.69717004\n",
      "Iteration 9, loss = 0.69570487\n",
      "Iteration 10, loss = 0.69429013\n",
      "Iteration 11, loss = 0.69292559\n",
      "Iteration 12, loss = 0.69161316\n",
      "Iteration 13, loss = 0.69035044\n",
      "Iteration 14, loss = 0.68913876\n",
      "Iteration 15, loss = 0.68796944\n",
      "Iteration 16, loss = 0.68683784\n",
      "Iteration 17, loss = 0.68574546\n",
      "Iteration 18, loss = 0.68469100\n",
      "Iteration 19, loss = 0.68366887\n",
      "Iteration 20, loss = 0.68266997\n",
      "Iteration 21, loss = 0.68169468\n",
      "Iteration 22, loss = 0.68073619\n",
      "Iteration 23, loss = 0.67978996\n",
      "Iteration 24, loss = 0.67886109\n",
      "Iteration 25, loss = 0.67794764\n",
      "Iteration 26, loss = 0.67703756\n",
      "Iteration 27, loss = 0.67612823\n",
      "Iteration 28, loss = 0.67521888\n",
      "Iteration 29, loss = 0.67430716\n",
      "Iteration 30, loss = 0.67339443\n",
      "Iteration 31, loss = 0.67247349\n",
      "Iteration 32, loss = 0.67154311\n",
      "Iteration 33, loss = 0.67060254\n",
      "Iteration 34, loss = 0.66964714\n",
      "Iteration 35, loss = 0.66868345\n",
      "Iteration 36, loss = 0.66771264\n",
      "Iteration 37, loss = 0.66673007\n",
      "Iteration 38, loss = 0.66573482\n",
      "Iteration 39, loss = 0.66472634\n",
      "Iteration 40, loss = 0.66370858\n",
      "Iteration 41, loss = 0.66268098\n",
      "Iteration 42, loss = 0.66164398\n",
      "Iteration 43, loss = 0.66059301\n",
      "Iteration 44, loss = 0.65952324\n",
      "Iteration 45, loss = 0.65843588\n",
      "Iteration 46, loss = 0.65733177\n",
      "Iteration 47, loss = 0.65620744\n",
      "Iteration 48, loss = 0.65507082\n",
      "Iteration 49, loss = 0.65392295\n",
      "Iteration 50, loss = 0.65276271\n",
      "Iteration 51, loss = 0.65158914\n",
      "Iteration 52, loss = 0.65040288\n",
      "Iteration 53, loss = 0.64920035\n",
      "Iteration 54, loss = 0.64798189\n",
      "Iteration 55, loss = 0.64674658\n",
      "Iteration 56, loss = 0.64549271\n",
      "Iteration 57, loss = 0.64423151\n",
      "Iteration 58, loss = 0.64294920\n",
      "Iteration 59, loss = 0.64165445\n",
      "Iteration 60, loss = 0.64034819\n",
      "Iteration 61, loss = 0.63902762\n",
      "Iteration 62, loss = 0.63769521\n",
      "Iteration 63, loss = 0.63635317\n",
      "Iteration 64, loss = 0.63500000\n",
      "Iteration 65, loss = 0.63363542\n",
      "Iteration 66, loss = 0.63226285\n",
      "Iteration 67, loss = 0.63087887\n",
      "Iteration 68, loss = 0.62948240\n",
      "Iteration 69, loss = 0.62807411\n",
      "Iteration 70, loss = 0.62665387\n",
      "Iteration 71, loss = 0.62522340\n",
      "Iteration 72, loss = 0.62378257\n",
      "Iteration 73, loss = 0.62233157\n",
      "Iteration 74, loss = 0.62087060\n",
      "Iteration 75, loss = 0.61940137\n",
      "Iteration 76, loss = 0.61792193\n",
      "Iteration 77, loss = 0.61643325\n",
      "Iteration 78, loss = 0.61493527\n",
      "Iteration 79, loss = 0.61342785\n",
      "Iteration 80, loss = 0.61191131\n",
      "Iteration 81, loss = 0.61038773\n",
      "Iteration 82, loss = 0.60885708\n",
      "Iteration 83, loss = 0.60731557\n",
      "Iteration 84, loss = 0.60576624\n",
      "Iteration 85, loss = 0.60420926\n",
      "Iteration 86, loss = 0.60264384\n",
      "Iteration 87, loss = 0.60107234\n",
      "Iteration 88, loss = 0.59949582\n",
      "Iteration 89, loss = 0.59791357\n",
      "Iteration 90, loss = 0.59632532\n",
      "Iteration 91, loss = 0.59473244\n",
      "Iteration 92, loss = 0.59313490\n",
      "Iteration 93, loss = 0.59153268\n",
      "Iteration 94, loss = 0.58992456\n",
      "Iteration 95, loss = 0.58831162\n",
      "Iteration 96, loss = 0.58669329\n",
      "Iteration 97, loss = 0.58507011\n",
      "Iteration 98, loss = 0.58344356\n",
      "Iteration 99, loss = 0.58181198\n",
      "Iteration 100, loss = 0.58017744\n",
      "Iteration 101, loss = 0.57854383\n",
      "Iteration 102, loss = 0.57690577\n",
      "Iteration 103, loss = 0.57526589\n",
      "Iteration 104, loss = 0.57362499\n",
      "Iteration 105, loss = 0.57198422\n",
      "Iteration 106, loss = 0.57033905\n",
      "Iteration 107, loss = 0.56869197\n",
      "Iteration 108, loss = 0.56704423\n",
      "Iteration 109, loss = 0.56539703\n",
      "Iteration 110, loss = 0.56374898\n",
      "Iteration 111, loss = 0.56209981\n",
      "Iteration 112, loss = 0.56045133\n",
      "Iteration 113, loss = 0.55880337\n",
      "Iteration 114, loss = 0.55715758\n",
      "Iteration 115, loss = 0.55551368\n",
      "Iteration 116, loss = 0.55387229\n",
      "Iteration 117, loss = 0.55223323\n",
      "Iteration 118, loss = 0.55059492\n",
      "Iteration 119, loss = 0.54895685\n",
      "Iteration 120, loss = 0.54732023\n",
      "Iteration 121, loss = 0.54568722\n",
      "Iteration 122, loss = 0.54405726\n",
      "Iteration 123, loss = 0.54242870\n",
      "Iteration 124, loss = 0.54080080\n",
      "Iteration 125, loss = 0.53917453\n",
      "Iteration 126, loss = 0.53755059\n",
      "Iteration 127, loss = 0.53592938\n",
      "Iteration 128, loss = 0.53431090\n",
      "Iteration 129, loss = 0.53269592\n",
      "Iteration 130, loss = 0.53108366\n",
      "Iteration 131, loss = 0.52947314\n",
      "Iteration 132, loss = 0.52786572\n",
      "Iteration 133, loss = 0.52626170\n",
      "Iteration 134, loss = 0.52466240\n",
      "Iteration 135, loss = 0.52306790\n",
      "Iteration 136, loss = 0.52147758\n",
      "Iteration 137, loss = 0.51989192\n",
      "Iteration 138, loss = 0.51831028\n",
      "Iteration 139, loss = 0.51673323\n",
      "Iteration 140, loss = 0.51516021\n",
      "Iteration 141, loss = 0.51359152\n",
      "Iteration 142, loss = 0.51202713\n",
      "Iteration 143, loss = 0.51046707\n",
      "Iteration 144, loss = 0.50891144\n",
      "Iteration 145, loss = 0.50736088\n",
      "Iteration 146, loss = 0.50581504\n",
      "Iteration 147, loss = 0.50427363\n",
      "Iteration 148, loss = 0.50273683\n",
      "Iteration 149, loss = 0.50120495\n",
      "Iteration 150, loss = 0.49967780\n",
      "Iteration 151, loss = 0.49815511\n",
      "Iteration 152, loss = 0.49663700\n",
      "Iteration 153, loss = 0.49512352\n",
      "Iteration 154, loss = 0.49361464\n",
      "Iteration 155, loss = 0.49211061\n",
      "Iteration 156, loss = 0.49061141\n",
      "Iteration 157, loss = 0.48911702\n",
      "Iteration 158, loss = 0.48762741\n",
      "Iteration 159, loss = 0.48614283\n",
      "Iteration 160, loss = 0.48466310\n",
      "Iteration 161, loss = 0.48318811\n",
      "Iteration 162, loss = 0.48171778\n",
      "Iteration 163, loss = 0.48025213\n",
      "Iteration 164, loss = 0.47879146\n",
      "Iteration 165, loss = 0.47733572\n",
      "Iteration 166, loss = 0.47588459\n",
      "Iteration 167, loss = 0.47443716\n",
      "Iteration 168, loss = 0.47299439\n",
      "Iteration 169, loss = 0.47155646\n",
      "Iteration 170, loss = 0.47012349\n",
      "Iteration 171, loss = 0.46869537\n",
      "Iteration 172, loss = 0.46727171\n",
      "Iteration 173, loss = 0.46585264\n",
      "Iteration 174, loss = 0.46443862\n",
      "Iteration 175, loss = 0.46302934\n",
      "Iteration 176, loss = 0.46162473\n",
      "Iteration 177, loss = 0.46022512\n",
      "Iteration 178, loss = 0.45883019\n",
      "Iteration 179, loss = 0.45743990\n",
      "Iteration 180, loss = 0.45605440\n",
      "Iteration 181, loss = 0.45467349\n",
      "Iteration 182, loss = 0.45329759\n",
      "Iteration 183, loss = 0.45192629\n",
      "Iteration 184, loss = 0.45055959\n",
      "Iteration 185, loss = 0.44919756\n",
      "Iteration 186, loss = 0.44784020\n",
      "Iteration 187, loss = 0.44648728\n",
      "Iteration 188, loss = 0.44513889\n",
      "Iteration 189, loss = 0.44379518\n",
      "Iteration 190, loss = 0.44245603\n",
      "Iteration 191, loss = 0.44112140\n",
      "Iteration 192, loss = 0.43979120\n",
      "Iteration 193, loss = 0.43846557\n",
      "Iteration 194, loss = 0.43714444\n",
      "Iteration 195, loss = 0.43582772\n",
      "Iteration 196, loss = 0.43451544\n",
      "Iteration 197, loss = 0.43320774\n",
      "Iteration 198, loss = 0.43190439\n",
      "Iteration 199, loss = 0.43060557\n",
      "Iteration 200, loss = 0.42931108\n",
      "Iteration 201, loss = 0.42802096\n",
      "Iteration 202, loss = 0.42673512\n",
      "Iteration 203, loss = 0.42545357\n",
      "Iteration 204, loss = 0.42417634\n",
      "Iteration 205, loss = 0.42290335\n",
      "Iteration 206, loss = 0.42163484\n",
      "Iteration 207, loss = 0.42037050\n",
      "Iteration 208, loss = 0.41911024\n",
      "Iteration 209, loss = 0.41785425\n",
      "Iteration 210, loss = 0.41660245\n",
      "Iteration 211, loss = 0.41535475\n",
      "Iteration 212, loss = 0.41411115\n",
      "Iteration 213, loss = 0.41287167\n",
      "Iteration 214, loss = 0.41163624\n",
      "Iteration 215, loss = 0.41040478\n",
      "Iteration 216, loss = 0.40917730\n",
      "Iteration 217, loss = 0.40795390\n",
      "Iteration 218, loss = 0.40673439\n",
      "Iteration 219, loss = 0.40551902\n",
      "Iteration 220, loss = 0.40430761\n",
      "Iteration 221, loss = 0.40310007\n",
      "Iteration 222, loss = 0.40189645\n",
      "Iteration 223, loss = 0.40069672\n",
      "Iteration 224, loss = 0.39950083\n",
      "Iteration 225, loss = 0.39830868\n",
      "Iteration 226, loss = 0.39712031\n",
      "Iteration 227, loss = 0.39593581\n",
      "Iteration 228, loss = 0.39475509\n",
      "Iteration 229, loss = 0.39357804\n",
      "Iteration 230, loss = 0.39240472\n",
      "Iteration 231, loss = 0.39123509\n",
      "Iteration 232, loss = 0.39006907\n",
      "Iteration 233, loss = 0.38890670\n",
      "Iteration 234, loss = 0.38774798\n",
      "Iteration 235, loss = 0.38659311\n",
      "Iteration 236, loss = 0.38544189\n",
      "Iteration 237, loss = 0.38429415\n",
      "Iteration 238, loss = 0.38314990\n",
      "Iteration 239, loss = 0.38200920\n",
      "Iteration 240, loss = 0.38087203\n",
      "Iteration 241, loss = 0.37973832\n",
      "Iteration 242, loss = 0.37860803\n",
      "Iteration 243, loss = 0.37748120\n",
      "Iteration 244, loss = 0.37635787\n",
      "Iteration 245, loss = 0.37523807\n",
      "Iteration 246, loss = 0.37412164\n",
      "Iteration 247, loss = 0.37300865\n",
      "Iteration 248, loss = 0.37189902\n",
      "Iteration 249, loss = 0.37079279\n",
      "Iteration 250, loss = 0.36968985\n",
      "Iteration 251, loss = 0.36859026\n",
      "Iteration 252, loss = 0.36749401\n",
      "Iteration 253, loss = 0.36640108\n",
      "Iteration 254, loss = 0.36531143\n",
      "Iteration 255, loss = 0.36422504\n",
      "Iteration 256, loss = 0.36314196\n",
      "Iteration 257, loss = 0.36206216\n",
      "Iteration 258, loss = 0.36098559\n",
      "Iteration 259, loss = 0.35991221\n",
      "Iteration 260, loss = 0.35884205\n",
      "Iteration 261, loss = 0.35777498\n",
      "Iteration 262, loss = 0.35671110\n",
      "Iteration 263, loss = 0.35565042\n",
      "Iteration 264, loss = 0.35459291\n",
      "Iteration 265, loss = 0.35353857\n",
      "Iteration 266, loss = 0.35248741\n",
      "Iteration 267, loss = 0.35143937\n",
      "Iteration 268, loss = 0.35039443\n",
      "Iteration 269, loss = 0.34935272\n",
      "Iteration 270, loss = 0.34831406\n",
      "Iteration 271, loss = 0.34727847\n",
      "Iteration 272, loss = 0.34624592\n",
      "Iteration 273, loss = 0.34521641\n",
      "Iteration 274, loss = 0.34418991\n",
      "Iteration 275, loss = 0.34316643\n",
      "Iteration 276, loss = 0.34214613\n",
      "Iteration 277, loss = 0.34112900\n",
      "Iteration 278, loss = 0.34011478\n",
      "Iteration 279, loss = 0.33910349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 280, loss = 0.33809519\n",
      "Iteration 281, loss = 0.33708982\n",
      "Iteration 282, loss = 0.33608738\n",
      "Iteration 283, loss = 0.33508787\n",
      "Iteration 284, loss = 0.33409139\n",
      "Iteration 285, loss = 0.33309790\n",
      "Iteration 286, loss = 0.33210731\n",
      "Iteration 287, loss = 0.33111966\n",
      "Iteration 288, loss = 0.33013502\n",
      "Iteration 289, loss = 0.32915325\n",
      "Iteration 290, loss = 0.32817431\n",
      "Iteration 291, loss = 0.32719834\n",
      "Iteration 292, loss = 0.32622525\n",
      "Iteration 293, loss = 0.32525499\n",
      "Iteration 294, loss = 0.32428767\n",
      "Iteration 295, loss = 0.32332314\n",
      "Iteration 296, loss = 0.32236142\n",
      "Iteration 297, loss = 0.32140251\n",
      "Iteration 298, loss = 0.32044640\n",
      "Iteration 299, loss = 0.31949308\n",
      "Iteration 300, loss = 0.31854255\n",
      "Iteration 301, loss = 0.31759479\n",
      "Iteration 302, loss = 0.31664978\n",
      "Iteration 303, loss = 0.31570754\n",
      "Iteration 304, loss = 0.31476806\n",
      "Iteration 305, loss = 0.31383131\n",
      "Iteration 306, loss = 0.31289728\n",
      "Iteration 307, loss = 0.31196602\n",
      "Iteration 308, loss = 0.31103749\n",
      "Iteration 309, loss = 0.31011167\n",
      "Iteration 310, loss = 0.30918858\n",
      "Iteration 311, loss = 0.30826828\n",
      "Iteration 312, loss = 0.30735074\n",
      "Iteration 313, loss = 0.30643588\n",
      "Iteration 314, loss = 0.30552365\n",
      "Iteration 315, loss = 0.30461408\n",
      "Iteration 316, loss = 0.30370715\n",
      "Iteration 317, loss = 0.30280288\n",
      "Iteration 318, loss = 0.30190123\n",
      "Iteration 319, loss = 0.30100218\n",
      "Iteration 320, loss = 0.30010573\n",
      "Iteration 321, loss = 0.29921189\n",
      "Iteration 322, loss = 0.29832066\n",
      "Iteration 323, loss = 0.29743204\n",
      "Iteration 324, loss = 0.29654609\n",
      "Iteration 325, loss = 0.29566268\n",
      "Iteration 326, loss = 0.29478181\n",
      "Iteration 327, loss = 0.29390348\n",
      "Iteration 328, loss = 0.29302771\n",
      "Iteration 329, loss = 0.29215451\n",
      "Iteration 330, loss = 0.29128383\n",
      "Iteration 331, loss = 0.29041569\n",
      "Iteration 332, loss = 0.28955006\n",
      "Iteration 333, loss = 0.28868694\n",
      "Iteration 334, loss = 0.28782632\n",
      "Iteration 335, loss = 0.28696820\n",
      "Iteration 336, loss = 0.28611257\n",
      "Iteration 337, loss = 0.28525942\n",
      "Iteration 338, loss = 0.28440875\n",
      "Iteration 339, loss = 0.28356054\n",
      "Iteration 340, loss = 0.28271489\n",
      "Iteration 341, loss = 0.28187174\n",
      "Iteration 342, loss = 0.28103102\n",
      "Iteration 343, loss = 0.28019272\n",
      "Iteration 344, loss = 0.27935683\n",
      "Iteration 345, loss = 0.27852345\n",
      "Iteration 346, loss = 0.27769257\n",
      "Iteration 347, loss = 0.27686413\n",
      "Iteration 348, loss = 0.27603807\n",
      "Iteration 349, loss = 0.27521439\n",
      "Iteration 350, loss = 0.27439310\n",
      "Iteration 351, loss = 0.27357420\n",
      "Iteration 352, loss = 0.27275767\n",
      "Iteration 353, loss = 0.27194350\n",
      "Iteration 354, loss = 0.27113168\n",
      "Iteration 355, loss = 0.27032220\n",
      "Iteration 356, loss = 0.26951507\n",
      "Iteration 357, loss = 0.26871029\n",
      "Iteration 358, loss = 0.26790781\n",
      "Iteration 359, loss = 0.26710766\n",
      "Iteration 360, loss = 0.26630989\n",
      "Iteration 361, loss = 0.26551443\n",
      "Iteration 362, loss = 0.26472129\n",
      "Iteration 363, loss = 0.26393046\n",
      "Iteration 364, loss = 0.26314195\n",
      "Iteration 365, loss = 0.26235572\n",
      "Iteration 366, loss = 0.26157177\n",
      "Iteration 367, loss = 0.26079010\n",
      "Iteration 368, loss = 0.26001072\n",
      "Iteration 369, loss = 0.25923362\n",
      "Iteration 370, loss = 0.25845878\n",
      "Iteration 371, loss = 0.25768620\n",
      "Iteration 372, loss = 0.25691587\n",
      "Iteration 373, loss = 0.25614780\n",
      "Iteration 374, loss = 0.25538196\n",
      "Iteration 375, loss = 0.25461835\n",
      "Iteration 376, loss = 0.25385699\n",
      "Iteration 377, loss = 0.25309790\n",
      "Iteration 378, loss = 0.25234100\n",
      "Iteration 379, loss = 0.25158634\n",
      "Iteration 380, loss = 0.25083390\n",
      "Iteration 381, loss = 0.25008367\n",
      "Iteration 382, loss = 0.24933561\n",
      "Iteration 383, loss = 0.24858975\n",
      "Iteration 384, loss = 0.24784608\n",
      "Iteration 385, loss = 0.24710464\n",
      "Iteration 386, loss = 0.24636530\n",
      "Iteration 387, loss = 0.24562806\n",
      "Iteration 388, loss = 0.24489302\n",
      "Iteration 389, loss = 0.24416012\n",
      "Iteration 390, loss = 0.24342935\n",
      "Iteration 391, loss = 0.24270078\n",
      "Iteration 392, loss = 0.24197438\n",
      "Iteration 393, loss = 0.24125008\n",
      "Iteration 394, loss = 0.24052789\n",
      "Iteration 395, loss = 0.23980782\n",
      "Iteration 396, loss = 0.23908986\n",
      "Iteration 397, loss = 0.23837399\n",
      "Iteration 398, loss = 0.23766040\n",
      "Iteration 399, loss = 0.23694874\n",
      "Iteration 400, loss = 0.23623917\n",
      "Iteration 401, loss = 0.23553175\n",
      "Iteration 402, loss = 0.23482639\n",
      "Iteration 403, loss = 0.23412308\n",
      "Iteration 404, loss = 0.23342180\n",
      "Iteration 405, loss = 0.23272256\n",
      "Iteration 406, loss = 0.23202539\n",
      "Iteration 407, loss = 0.23133031\n",
      "Iteration 408, loss = 0.23063729\n",
      "Iteration 409, loss = 0.22994621\n",
      "Iteration 410, loss = 0.22925726\n",
      "Iteration 411, loss = 0.22857029\n",
      "Iteration 412, loss = 0.22788542\n",
      "Iteration 413, loss = 0.22720247\n",
      "Iteration 414, loss = 0.22652160\n",
      "Iteration 415, loss = 0.22584264\n",
      "Iteration 416, loss = 0.22516573\n",
      "Iteration 417, loss = 0.22449082\n",
      "Iteration 418, loss = 0.22381791\n",
      "Iteration 419, loss = 0.22314700\n",
      "Iteration 420, loss = 0.22247810\n",
      "Iteration 421, loss = 0.22181117\n",
      "Iteration 422, loss = 0.22114622\n",
      "Iteration 423, loss = 0.22048325\n",
      "Iteration 424, loss = 0.21982220\n",
      "Iteration 425, loss = 0.21916320\n",
      "Iteration 426, loss = 0.21850597\n",
      "Iteration 427, loss = 0.21785081\n",
      "Iteration 428, loss = 0.21719754\n",
      "Iteration 429, loss = 0.21654624\n",
      "Iteration 430, loss = 0.21589687\n",
      "Iteration 431, loss = 0.21524942\n",
      "Iteration 432, loss = 0.21460388\n",
      "Iteration 433, loss = 0.21396027\n",
      "Iteration 434, loss = 0.21331866\n",
      "Iteration 435, loss = 0.21267889\n",
      "Iteration 436, loss = 0.21204088\n",
      "Iteration 437, loss = 0.21140497\n",
      "Iteration 438, loss = 0.21077083\n",
      "Iteration 439, loss = 0.21013863\n",
      "Iteration 440, loss = 0.20950833\n",
      "Iteration 441, loss = 0.20887983\n",
      "Iteration 442, loss = 0.20825323\n",
      "Iteration 443, loss = 0.20762857\n",
      "Iteration 444, loss = 0.20700569\n",
      "Iteration 445, loss = 0.20638468\n",
      "Iteration 446, loss = 0.20576555\n",
      "Iteration 447, loss = 0.20514836\n",
      "Iteration 448, loss = 0.20453299\n",
      "Iteration 449, loss = 0.20391955\n",
      "Iteration 450, loss = 0.20330797\n",
      "Iteration 451, loss = 0.20269820\n",
      "Iteration 452, loss = 0.20209023\n",
      "Iteration 453, loss = 0.20148414\n",
      "Iteration 454, loss = 0.20087984\n",
      "Iteration 455, loss = 0.20027732\n",
      "Iteration 456, loss = 0.19967664\n",
      "Iteration 457, loss = 0.19907777\n",
      "Iteration 458, loss = 0.19848068\n",
      "Iteration 459, loss = 0.19788544\n",
      "Iteration 460, loss = 0.19729199\n",
      "Iteration 461, loss = 0.19670030\n",
      "Iteration 462, loss = 0.19611037\n",
      "Iteration 463, loss = 0.19552226\n",
      "Iteration 464, loss = 0.19493592\n",
      "Iteration 465, loss = 0.19435135\n",
      "Iteration 466, loss = 0.19376857\n",
      "Iteration 467, loss = 0.19318785\n",
      "Iteration 468, loss = 0.19260874\n",
      "Iteration 469, loss = 0.19203160\n",
      "Iteration 470, loss = 0.19145639\n",
      "Iteration 471, loss = 0.19088257\n",
      "Iteration 472, loss = 0.19031052\n",
      "Iteration 473, loss = 0.18974012\n",
      "Iteration 474, loss = 0.18917148\n",
      "Iteration 475, loss = 0.18860469\n",
      "Iteration 476, loss = 0.18803938\n",
      "Iteration 477, loss = 0.18747593\n",
      "Iteration 478, loss = 0.18691437\n",
      "Iteration 479, loss = 0.18635423\n",
      "Iteration 480, loss = 0.18579606\n",
      "Iteration 481, loss = 0.18523965\n",
      "Iteration 482, loss = 0.18468463\n",
      "Iteration 483, loss = 0.18413158\n",
      "Iteration 484, loss = 0.18358010\n",
      "Iteration 485, loss = 0.18303020\n",
      "Iteration 486, loss = 0.18248222\n",
      "Iteration 487, loss = 0.18193579\n",
      "Iteration 488, loss = 0.18139097\n",
      "Iteration 489, loss = 0.18084792\n",
      "Iteration 490, loss = 0.18030660\n",
      "Iteration 491, loss = 0.17976682\n",
      "Iteration 492, loss = 0.17922883\n",
      "Iteration 493, loss = 0.17869236\n",
      "Iteration 494, loss = 0.17815773\n",
      "Iteration 495, loss = 0.17762449\n",
      "Iteration 496, loss = 0.17709337\n",
      "Iteration 497, loss = 0.17656351\n",
      "Iteration 498, loss = 0.17603506\n",
      "Iteration 499, loss = 0.17550855\n",
      "Iteration 500, loss = 0.17498365\n",
      "Iteration 501, loss = 0.17446044\n",
      "Iteration 502, loss = 0.17393865\n",
      "Iteration 503, loss = 0.17341856\n",
      "Iteration 504, loss = 0.17290003\n",
      "Iteration 505, loss = 0.17238326\n",
      "Iteration 506, loss = 0.17186804\n",
      "Iteration 507, loss = 0.17135433\n",
      "Iteration 508, loss = 0.17084229\n",
      "Iteration 509, loss = 0.17033181\n",
      "Iteration 510, loss = 0.16982286\n",
      "Iteration 511, loss = 0.16931557\n",
      "Iteration 512, loss = 0.16880984\n",
      "Iteration 513, loss = 0.16830573\n",
      "Iteration 514, loss = 0.16780304\n",
      "Iteration 515, loss = 0.16730217\n",
      "Iteration 516, loss = 0.16680260\n",
      "Iteration 517, loss = 0.16630484\n",
      "Iteration 518, loss = 0.16580835\n",
      "Iteration 519, loss = 0.16531362\n",
      "Iteration 520, loss = 0.16482029\n",
      "Iteration 521, loss = 0.16432872\n",
      "Iteration 522, loss = 0.16383856\n",
      "Iteration 523, loss = 0.16334995\n",
      "Iteration 524, loss = 0.16286292\n",
      "Iteration 525, loss = 0.16237737\n",
      "Iteration 526, loss = 0.16189331\n",
      "Iteration 527, loss = 0.16141082\n",
      "Iteration 528, loss = 0.16092990\n",
      "Iteration 529, loss = 0.16045042\n",
      "Iteration 530, loss = 0.15997248\n",
      "Iteration 531, loss = 0.15949607\n",
      "Iteration 532, loss = 0.15902111\n",
      "Iteration 533, loss = 0.15854763\n",
      "Iteration 534, loss = 0.15807572\n",
      "Iteration 535, loss = 0.15760528\n",
      "Iteration 536, loss = 0.15713643\n",
      "Iteration 537, loss = 0.15666891\n",
      "Iteration 538, loss = 0.15620289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 539, loss = 0.15573834\n",
      "Iteration 540, loss = 0.15527531\n",
      "Iteration 541, loss = 0.15481369\n",
      "Iteration 542, loss = 0.15435370\n",
      "Iteration 543, loss = 0.15389491\n",
      "Iteration 544, loss = 0.15343785\n",
      "Iteration 545, loss = 0.15298202\n",
      "Iteration 546, loss = 0.15252789\n",
      "Iteration 547, loss = 0.15207497\n",
      "Iteration 548, loss = 0.15162365\n",
      "Iteration 549, loss = 0.15117379\n",
      "Iteration 550, loss = 0.15072505\n",
      "Iteration 551, loss = 0.15027819\n",
      "Iteration 552, loss = 0.14983248\n",
      "Iteration 553, loss = 0.14938817\n",
      "Iteration 554, loss = 0.14894552\n",
      "Iteration 555, loss = 0.14850410\n",
      "Iteration 556, loss = 0.14806398\n",
      "Iteration 557, loss = 0.14762544\n",
      "Iteration 558, loss = 0.14718839\n",
      "Iteration 559, loss = 0.14675250\n",
      "Iteration 560, loss = 0.14631806\n",
      "Iteration 561, loss = 0.14588517\n",
      "Iteration 562, loss = 0.14545356\n",
      "Iteration 563, loss = 0.14502328\n",
      "Iteration 564, loss = 0.14459447\n",
      "Iteration 565, loss = 0.14416713\n",
      "Iteration 566, loss = 0.14374105\n",
      "Iteration 567, loss = 0.14331632\n",
      "Iteration 568, loss = 0.14289299\n",
      "Iteration 569, loss = 0.14247111\n",
      "Iteration 570, loss = 0.14205050\n",
      "Iteration 571, loss = 0.14163121\n",
      "Iteration 572, loss = 0.14121350\n",
      "Iteration 573, loss = 0.14079698\n",
      "Iteration 574, loss = 0.14038165\n",
      "Iteration 575, loss = 0.13996787\n",
      "Iteration 576, loss = 0.13955536\n",
      "Iteration 577, loss = 0.13914420\n",
      "Iteration 578, loss = 0.13873441\n",
      "Iteration 579, loss = 0.13832588\n",
      "Iteration 580, loss = 0.13791870\n",
      "Iteration 581, loss = 0.13751286\n",
      "Iteration 582, loss = 0.13710832\n",
      "Iteration 583, loss = 0.13670515\n",
      "Iteration 584, loss = 0.13630349\n",
      "Iteration 585, loss = 0.13590280\n",
      "Iteration 586, loss = 0.13550352\n",
      "Iteration 587, loss = 0.13510541\n",
      "Iteration 588, loss = 0.13470875\n",
      "Iteration 589, loss = 0.13431339\n",
      "Iteration 590, loss = 0.13391925\n",
      "Iteration 591, loss = 0.13352641\n",
      "Iteration 592, loss = 0.13313488\n",
      "Iteration 593, loss = 0.13274459\n",
      "Iteration 594, loss = 0.13235560\n",
      "Iteration 595, loss = 0.13196792\n",
      "Iteration 596, loss = 0.13158145\n",
      "Iteration 597, loss = 0.13119634\n",
      "Iteration 598, loss = 0.13081239\n",
      "Iteration 599, loss = 0.13042971\n",
      "Iteration 600, loss = 0.13004838\n",
      "Iteration 601, loss = 0.12966813\n",
      "Iteration 602, loss = 0.12928931\n",
      "Iteration 603, loss = 0.12891163\n",
      "Iteration 604, loss = 0.12853516\n",
      "Iteration 605, loss = 0.12816006\n",
      "Iteration 606, loss = 0.12778611\n",
      "Iteration 607, loss = 0.12741332\n",
      "Iteration 608, loss = 0.12704184\n",
      "Iteration 609, loss = 0.12667156\n",
      "Iteration 610, loss = 0.12630248\n",
      "Iteration 611, loss = 0.12593463\n",
      "Iteration 612, loss = 0.12556800\n",
      "Iteration 613, loss = 0.12520257\n",
      "Iteration 614, loss = 0.12483834\n",
      "Iteration 615, loss = 0.12447530\n",
      "Iteration 616, loss = 0.12411346\n",
      "Iteration 617, loss = 0.12375289\n",
      "Iteration 618, loss = 0.12339337\n",
      "Iteration 619, loss = 0.12303524\n",
      "Iteration 620, loss = 0.12267818\n",
      "Iteration 621, loss = 0.12232218\n",
      "Iteration 622, loss = 0.12196751\n",
      "Iteration 623, loss = 0.12161399\n",
      "Iteration 624, loss = 0.12126160\n",
      "Iteration 625, loss = 0.12091039\n",
      "Iteration 626, loss = 0.12056035\n",
      "Iteration 627, loss = 0.12021147\n",
      "Iteration 628, loss = 0.11986374\n",
      "Iteration 629, loss = 0.11951723\n",
      "Iteration 630, loss = 0.11917176\n",
      "Iteration 631, loss = 0.11882752\n",
      "Iteration 632, loss = 0.11848438\n",
      "Iteration 633, loss = 0.11814233\n",
      "Iteration 634, loss = 0.11780149\n",
      "Iteration 635, loss = 0.11746174\n",
      "Iteration 636, loss = 0.11712318\n",
      "Iteration 637, loss = 0.11678570\n",
      "Iteration 638, loss = 0.11644930\n",
      "Iteration 639, loss = 0.11611410\n",
      "Iteration 640, loss = 0.11577997\n",
      "Iteration 641, loss = 0.11544691\n",
      "Iteration 642, loss = 0.11511507\n",
      "Iteration 643, loss = 0.11478429\n",
      "Iteration 644, loss = 0.11445453\n",
      "Iteration 645, loss = 0.11412590\n",
      "Iteration 646, loss = 0.11379842\n",
      "Iteration 647, loss = 0.11347199\n",
      "Iteration 648, loss = 0.11314664\n",
      "Iteration 649, loss = 0.11282236\n",
      "Iteration 650, loss = 0.11249915\n",
      "Iteration 651, loss = 0.11217710\n",
      "Iteration 652, loss = 0.11185607\n",
      "Iteration 653, loss = 0.11153606\n",
      "Iteration 654, loss = 0.11121724\n",
      "Iteration 655, loss = 0.11089938\n",
      "Iteration 656, loss = 0.11058250\n",
      "Iteration 657, loss = 0.11026679\n",
      "Iteration 658, loss = 0.10995210\n",
      "Iteration 659, loss = 0.10963846\n",
      "Iteration 660, loss = 0.10932589\n",
      "Iteration 661, loss = 0.10901435\n",
      "Iteration 662, loss = 0.10870382\n",
      "Iteration 663, loss = 0.10839431\n",
      "Iteration 664, loss = 0.10808582\n",
      "Iteration 665, loss = 0.10777843\n",
      "Iteration 666, loss = 0.10747202\n",
      "Iteration 667, loss = 0.10716657\n",
      "Iteration 668, loss = 0.10686221\n",
      "Iteration 669, loss = 0.10655885\n",
      "Iteration 670, loss = 0.10625651\n",
      "Iteration 671, loss = 0.10595517\n",
      "Iteration 672, loss = 0.10565487\n",
      "Iteration 673, loss = 0.10535552\n",
      "Iteration 674, loss = 0.10505720\n",
      "Iteration 675, loss = 0.10475983\n",
      "Iteration 676, loss = 0.10446349\n",
      "Iteration 677, loss = 0.10416811\n",
      "Iteration 678, loss = 0.10387372\n",
      "Iteration 679, loss = 0.10358033\n",
      "Iteration 680, loss = 0.10328788\n",
      "Iteration 681, loss = 0.10299641\n",
      "Iteration 682, loss = 0.10270599\n",
      "Iteration 683, loss = 0.10241645\n",
      "Iteration 684, loss = 0.10212790\n",
      "Iteration 685, loss = 0.10184032\n",
      "Iteration 686, loss = 0.10155363\n",
      "Iteration 687, loss = 0.10126796\n",
      "Iteration 688, loss = 0.10098322\n",
      "Iteration 689, loss = 0.10069939\n",
      "Iteration 690, loss = 0.10041658\n",
      "Iteration 691, loss = 0.10013468\n",
      "Iteration 692, loss = 0.09985371\n",
      "Iteration 693, loss = 0.09957364\n",
      "Iteration 694, loss = 0.09929451\n",
      "Iteration 695, loss = 0.09901639\n",
      "Iteration 696, loss = 0.09873910\n",
      "Iteration 697, loss = 0.09846282\n",
      "Iteration 698, loss = 0.09818743\n",
      "Iteration 699, loss = 0.09791284\n",
      "Iteration 700, loss = 0.09763931\n",
      "Iteration 701, loss = 0.09736666\n",
      "Iteration 702, loss = 0.09709481\n",
      "Iteration 703, loss = 0.09682393\n",
      "Iteration 704, loss = 0.09655399\n",
      "Iteration 705, loss = 0.09628493\n",
      "Iteration 706, loss = 0.09601674\n",
      "Iteration 707, loss = 0.09574944\n",
      "Iteration 708, loss = 0.09548301\n",
      "Iteration 709, loss = 0.09521745\n",
      "Iteration 710, loss = 0.09495277\n",
      "Iteration 711, loss = 0.09468900\n",
      "Iteration 712, loss = 0.09442612\n",
      "Iteration 713, loss = 0.09416405\n",
      "Iteration 714, loss = 0.09390291\n",
      "Iteration 715, loss = 0.09364264\n",
      "Iteration 716, loss = 0.09338319\n",
      "Iteration 717, loss = 0.09312460\n",
      "Iteration 718, loss = 0.09286696\n",
      "Iteration 719, loss = 0.09261010\n",
      "Iteration 720, loss = 0.09235402\n",
      "Iteration 721, loss = 0.09209887\n",
      "Iteration 722, loss = 0.09184457\n",
      "Iteration 723, loss = 0.09159109\n",
      "Iteration 724, loss = 0.09133844\n",
      "Iteration 725, loss = 0.09108663\n",
      "Iteration 726, loss = 0.09083570\n",
      "Iteration 727, loss = 0.09058554\n",
      "Iteration 728, loss = 0.09033623\n",
      "Iteration 729, loss = 0.09008776\n",
      "Iteration 730, loss = 0.08984007\n",
      "Iteration 731, loss = 0.08959322\n",
      "Iteration 732, loss = 0.08934723\n",
      "Iteration 733, loss = 0.08910201\n",
      "Iteration 734, loss = 0.08885759\n",
      "Iteration 735, loss = 0.08861401\n",
      "Iteration 736, loss = 0.08837122\n",
      "Iteration 737, loss = 0.08812922\n",
      "Iteration 738, loss = 0.08788804\n",
      "Iteration 739, loss = 0.08764766\n",
      "Iteration 740, loss = 0.08740806\n",
      "Iteration 741, loss = 0.08716927\n",
      "Iteration 742, loss = 0.08693124\n",
      "Iteration 743, loss = 0.08669400\n",
      "Iteration 744, loss = 0.08645757\n",
      "Iteration 745, loss = 0.08622190\n",
      "Iteration 746, loss = 0.08598701\n",
      "Iteration 747, loss = 0.08575289\n",
      "Iteration 748, loss = 0.08551956\n",
      "Iteration 749, loss = 0.08528696\n",
      "Iteration 750, loss = 0.08505515\n",
      "Iteration 751, loss = 0.08482411\n",
      "Iteration 752, loss = 0.08459381\n",
      "Iteration 753, loss = 0.08436429\n",
      "Iteration 754, loss = 0.08413554\n",
      "Iteration 755, loss = 0.08390751\n",
      "Iteration 756, loss = 0.08368025\n",
      "Iteration 757, loss = 0.08345374\n",
      "Iteration 758, loss = 0.08322795\n",
      "Iteration 759, loss = 0.08300292\n",
      "Iteration 760, loss = 0.08277862\n",
      "Iteration 761, loss = 0.08255506\n",
      "Iteration 762, loss = 0.08233224\n",
      "Iteration 763, loss = 0.08211014\n",
      "Iteration 764, loss = 0.08188878\n",
      "Iteration 765, loss = 0.08166817\n",
      "Iteration 766, loss = 0.08144825\n",
      "Iteration 767, loss = 0.08122907\n",
      "Iteration 768, loss = 0.08101059\n",
      "Iteration 769, loss = 0.08079284\n",
      "Iteration 770, loss = 0.08057582\n",
      "Iteration 771, loss = 0.08035949\n",
      "Iteration 772, loss = 0.08014391\n",
      "Iteration 773, loss = 0.07992902\n",
      "Iteration 774, loss = 0.07971482\n",
      "Iteration 775, loss = 0.07950136\n",
      "Iteration 776, loss = 0.07928861\n",
      "Iteration 777, loss = 0.07907647\n",
      "Iteration 778, loss = 0.07886506\n",
      "Iteration 779, loss = 0.07865439\n",
      "Iteration 780, loss = 0.07844435\n",
      "Iteration 781, loss = 0.07823497\n",
      "Iteration 782, loss = 0.07802641\n",
      "Iteration 783, loss = 0.07781835\n",
      "Iteration 784, loss = 0.07761119\n",
      "Iteration 785, loss = 0.07740464\n",
      "Iteration 786, loss = 0.07719870\n",
      "Iteration 787, loss = 0.07699338\n",
      "Iteration 788, loss = 0.07678872\n",
      "Iteration 789, loss = 0.07658489\n",
      "Iteration 790, loss = 0.07638162\n",
      "Iteration 791, loss = 0.07617898\n",
      "Iteration 792, loss = 0.07597707\n",
      "Iteration 793, loss = 0.07577583\n",
      "Iteration 794, loss = 0.07557517\n",
      "Iteration 795, loss = 0.07537521\n",
      "Iteration 796, loss = 0.07517594\n",
      "Iteration 797, loss = 0.07497725\n",
      "Iteration 798, loss = 0.07477924\n",
      "Iteration 799, loss = 0.07458190\n",
      "Iteration 800, loss = 0.07438514\n",
      "Iteration 801, loss = 0.07418908\n",
      "Iteration 802, loss = 0.07399365\n",
      "Iteration 803, loss = 0.07379879\n",
      "Iteration 804, loss = 0.07360465\n",
      "Iteration 805, loss = 0.07341113\n",
      "Iteration 806, loss = 0.07321817\n",
      "Iteration 807, loss = 0.07302592\n",
      "Iteration 808, loss = 0.07283425\n",
      "Iteration 809, loss = 0.07264320\n",
      "Iteration 810, loss = 0.07245281\n",
      "Iteration 811, loss = 0.07226297\n",
      "Iteration 812, loss = 0.07207379\n",
      "Iteration 813, loss = 0.07188522\n",
      "Iteration 814, loss = 0.07169734\n",
      "Iteration 815, loss = 0.07151004\n",
      "Iteration 816, loss = 0.07132328\n",
      "Iteration 817, loss = 0.07113705\n",
      "Iteration 818, loss = 0.07095174\n",
      "Iteration 819, loss = 0.07076677\n",
      "Iteration 820, loss = 0.07058230\n",
      "Iteration 821, loss = 0.07039865\n",
      "Iteration 822, loss = 0.07021552\n",
      "Iteration 823, loss = 0.07003289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 824, loss = 0.06985100\n",
      "Iteration 825, loss = 0.06966975\n",
      "Iteration 826, loss = 0.06948882\n",
      "Iteration 827, loss = 0.06930862\n",
      "Iteration 828, loss = 0.06912905\n",
      "Iteration 829, loss = 0.06894998\n",
      "Iteration 830, loss = 0.06877151\n",
      "Iteration 831, loss = 0.06859367\n",
      "Iteration 832, loss = 0.06841633\n",
      "Iteration 833, loss = 0.06823960\n",
      "Iteration 834, loss = 0.06806343\n",
      "Iteration 835, loss = 0.06788784\n",
      "Iteration 836, loss = 0.06771279\n",
      "Iteration 837, loss = 0.06753831\n",
      "Iteration 838, loss = 0.06736443\n",
      "Iteration 839, loss = 0.06719112\n",
      "Iteration 840, loss = 0.06701839\n",
      "Iteration 841, loss = 0.06684613\n",
      "Iteration 842, loss = 0.06667441\n",
      "Iteration 843, loss = 0.06650337\n",
      "Iteration 844, loss = 0.06633273\n",
      "Iteration 845, loss = 0.06616271\n",
      "Iteration 846, loss = 0.06599325\n",
      "Iteration 847, loss = 0.06582427\n",
      "Iteration 848, loss = 0.06565598\n",
      "Iteration 849, loss = 0.06548805\n",
      "Iteration 850, loss = 0.06532081\n",
      "Iteration 851, loss = 0.06515414\n",
      "Iteration 852, loss = 0.06498789\n",
      "Iteration 853, loss = 0.06482208\n",
      "Iteration 854, loss = 0.06465686\n",
      "Iteration 855, loss = 0.06449228\n",
      "Iteration 856, loss = 0.06432812\n",
      "Iteration 857, loss = 0.06416449\n",
      "Iteration 858, loss = 0.06400144\n",
      "Iteration 859, loss = 0.06383889\n",
      "Iteration 860, loss = 0.06367684\n",
      "Iteration 861, loss = 0.06351536\n",
      "Iteration 862, loss = 0.06335439\n",
      "Iteration 863, loss = 0.06319392\n",
      "Iteration 864, loss = 0.06303395\n",
      "Iteration 865, loss = 0.06287451\n",
      "Iteration 866, loss = 0.06271551\n",
      "Iteration 867, loss = 0.06255720\n",
      "Iteration 868, loss = 0.06239922\n",
      "Iteration 869, loss = 0.06224178\n",
      "Iteration 870, loss = 0.06208493\n",
      "Iteration 871, loss = 0.06192851\n",
      "Iteration 872, loss = 0.06177250\n",
      "Iteration 873, loss = 0.06161710\n",
      "Iteration 874, loss = 0.06146227\n",
      "Iteration 875, loss = 0.06130771\n",
      "Iteration 876, loss = 0.06115376\n",
      "Iteration 877, loss = 0.06100037\n",
      "Iteration 878, loss = 0.06084740\n",
      "Iteration 879, loss = 0.06069485\n",
      "Iteration 880, loss = 0.06054287\n",
      "Iteration 881, loss = 0.06039136\n",
      "Iteration 882, loss = 0.06024028\n",
      "Iteration 883, loss = 0.06008976\n",
      "Iteration 884, loss = 0.05993967\n",
      "Iteration 885, loss = 0.05979006\n",
      "Iteration 886, loss = 0.05964093\n",
      "Iteration 887, loss = 0.05949226\n",
      "Iteration 888, loss = 0.05934414\n",
      "Iteration 889, loss = 0.05919643\n",
      "Iteration 890, loss = 0.05904914\n",
      "Iteration 891, loss = 0.05890249\n",
      "Iteration 892, loss = 0.05875615\n",
      "Iteration 893, loss = 0.05861022\n",
      "Iteration 894, loss = 0.05846490\n",
      "Iteration 895, loss = 0.05831996\n",
      "Iteration 896, loss = 0.05817541\n",
      "Iteration 897, loss = 0.05803153\n",
      "Iteration 898, loss = 0.05788794\n",
      "Iteration 899, loss = 0.05774478\n",
      "Iteration 900, loss = 0.05760217\n",
      "Iteration 901, loss = 0.05745995\n",
      "Iteration 902, loss = 0.05731817\n",
      "Iteration 903, loss = 0.05717690\n",
      "Iteration 904, loss = 0.05703604\n",
      "Iteration 905, loss = 0.05689568\n",
      "Iteration 906, loss = 0.05675568\n",
      "Iteration 907, loss = 0.05661611\n",
      "Iteration 908, loss = 0.05647706\n",
      "Iteration 909, loss = 0.05633838\n",
      "Iteration 910, loss = 0.05620021\n",
      "Iteration 911, loss = 0.05606243\n",
      "Iteration 912, loss = 0.05592505\n",
      "Iteration 913, loss = 0.05578818\n",
      "Iteration 914, loss = 0.05565166\n",
      "Iteration 915, loss = 0.05551567\n",
      "Iteration 916, loss = 0.05538005\n",
      "Iteration 917, loss = 0.05524480\n",
      "Iteration 918, loss = 0.05511004\n",
      "Iteration 919, loss = 0.05497568\n",
      "Iteration 920, loss = 0.05484175\n",
      "Iteration 921, loss = 0.05470827\n",
      "Iteration 922, loss = 0.05457516\n",
      "Iteration 923, loss = 0.05444244\n",
      "Iteration 924, loss = 0.05431032\n",
      "Iteration 925, loss = 0.05417839\n",
      "Iteration 926, loss = 0.05404697\n",
      "Iteration 927, loss = 0.05391604\n",
      "Iteration 928, loss = 0.05378544\n",
      "Iteration 929, loss = 0.05365519\n",
      "Iteration 930, loss = 0.05352531\n",
      "Iteration 931, loss = 0.05339595\n",
      "Iteration 932, loss = 0.05326694\n",
      "Iteration 933, loss = 0.05313836\n",
      "Iteration 934, loss = 0.05301021\n",
      "Iteration 935, loss = 0.05288241\n",
      "Iteration 936, loss = 0.05275499\n",
      "Iteration 937, loss = 0.05262809\n",
      "Iteration 938, loss = 0.05250146\n",
      "Iteration 939, loss = 0.05237524\n",
      "Iteration 940, loss = 0.05224950\n",
      "Iteration 941, loss = 0.05212407\n",
      "Iteration 942, loss = 0.05199898\n",
      "Iteration 943, loss = 0.05187452\n",
      "Iteration 944, loss = 0.05175027\n",
      "Iteration 945, loss = 0.05162624\n",
      "Iteration 946, loss = 0.05150284\n",
      "Iteration 947, loss = 0.05137975\n",
      "Iteration 948, loss = 0.05125699\n",
      "Iteration 949, loss = 0.05113470\n",
      "Iteration 950, loss = 0.05101277\n",
      "Iteration 951, loss = 0.05089116\n",
      "Iteration 952, loss = 0.05077004\n",
      "Iteration 953, loss = 0.05064923\n",
      "Iteration 954, loss = 0.05052874\n",
      "Iteration 955, loss = 0.05040870\n",
      "Iteration 956, loss = 0.05028902\n",
      "Iteration 957, loss = 0.05016964\n",
      "Iteration 958, loss = 0.05005075\n",
      "Iteration 959, loss = 0.04993218\n",
      "Iteration 960, loss = 0.04981391\n",
      "Iteration 961, loss = 0.04969602\n",
      "Iteration 962, loss = 0.04957851\n",
      "Iteration 963, loss = 0.04946139\n",
      "Iteration 964, loss = 0.04934461\n",
      "Iteration 965, loss = 0.04922822\n",
      "Iteration 966, loss = 0.04911218\n",
      "Iteration 967, loss = 0.04899649\n",
      "Iteration 968, loss = 0.04888113\n",
      "Iteration 969, loss = 0.04876616\n",
      "Iteration 970, loss = 0.04865151\n",
      "Iteration 971, loss = 0.04853724\n",
      "Iteration 972, loss = 0.04842336\n",
      "Iteration 973, loss = 0.04830978\n",
      "Iteration 974, loss = 0.04819657\n",
      "Iteration 975, loss = 0.04808368\n",
      "Iteration 976, loss = 0.04797116\n",
      "Iteration 977, loss = 0.04785897\n",
      "Iteration 978, loss = 0.04774713\n",
      "Iteration 979, loss = 0.04763562\n",
      "Iteration 980, loss = 0.04752447\n",
      "Iteration 981, loss = 0.04741368\n",
      "Iteration 982, loss = 0.04730320\n",
      "Iteration 983, loss = 0.04719305\n",
      "Iteration 984, loss = 0.04708334\n",
      "Iteration 985, loss = 0.04697382\n",
      "Iteration 986, loss = 0.04686473\n",
      "Iteration 987, loss = 0.04675597\n",
      "Iteration 988, loss = 0.04664748\n",
      "Iteration 989, loss = 0.04653930\n",
      "Iteration 990, loss = 0.04643154\n",
      "Iteration 991, loss = 0.04632406\n",
      "Iteration 992, loss = 0.04621694\n",
      "Iteration 993, loss = 0.04611011\n",
      "Iteration 994, loss = 0.04600364\n",
      "Iteration 995, loss = 0.04589750\n",
      "Iteration 996, loss = 0.04579161\n",
      "Iteration 997, loss = 0.04568612\n",
      "Iteration 998, loss = 0.04558091\n",
      "Iteration 999, loss = 0.04547602\n",
      "Iteration 1000, loss = 0.04537150\n",
      "Iteration 1001, loss = 0.04526721\n",
      "Iteration 1002, loss = 0.04516332\n",
      "Iteration 1003, loss = 0.04505971\n",
      "Iteration 1004, loss = 0.04495644\n",
      "Iteration 1005, loss = 0.04485345\n",
      "Iteration 1006, loss = 0.04475081\n",
      "Iteration 1007, loss = 0.04464848\n",
      "Iteration 1008, loss = 0.04454639\n",
      "Iteration 1009, loss = 0.04444471\n",
      "Iteration 1010, loss = 0.04434322\n",
      "Iteration 1011, loss = 0.04424215\n",
      "Iteration 1012, loss = 0.04414138\n",
      "Iteration 1013, loss = 0.04404083\n",
      "Iteration 1014, loss = 0.04394060\n",
      "Iteration 1015, loss = 0.04384082\n",
      "Iteration 1016, loss = 0.04374116\n",
      "Iteration 1017, loss = 0.04364184\n",
      "Iteration 1018, loss = 0.04354291\n",
      "Iteration 1019, loss = 0.04344422\n",
      "Iteration 1020, loss = 0.04334577\n",
      "Iteration 1021, loss = 0.04324766\n",
      "Iteration 1022, loss = 0.04314992\n",
      "Iteration 1023, loss = 0.04305244\n",
      "Iteration 1024, loss = 0.04295514\n",
      "Iteration 1025, loss = 0.04285822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=600000, verbose=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = np.array(histo_list)\n",
    "Y = [tup[0] for tup in X_list]\n",
    "\n",
    "# It's a way to convert species name into an integer\n",
    "#for s in train.species:\n",
    "    #Y.append(np.min(np.nonzero(species == s)))\n",
    "\n",
    "mlp = MLPClassifier(verbose=True, max_iter=600000)\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "300d6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_list = []\n",
    "for i, class_ in enumerate(classes_list):\n",
    "    image_names = os. listdir('./samples_test/' + class_)\n",
    "\n",
    "    for image_name in images_names:\n",
    "        image_path = './samples_test/' + class_ + '/'+ image_name\n",
    "        image_class = i\n",
    "        class_name = class_\n",
    "        X_test_list.append((image_class, class_name, image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01fe90c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 136)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-16a16c4d72f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mkp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "#result_file = open(\"sift.csv\", \"w\")\n",
    "#result_file_obj = csv.writer(result_file)\n",
    "#result_file_obj.writerow(np.append(\"id\", species))\n",
    "\n",
    "for tup in X_test_list:\n",
    "    img = cv2.imread(tup[2],0)\n",
    "    print(img.shape)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "\n",
    "    x = np.zeros(k)\n",
    "    nkp = np.size(kp)\n",
    "\n",
    "    for d in des:\n",
    "        idx = kmeans.predict([d])\n",
    "        x[idx] += 1/nkp\n",
    "\n",
    "    res = mlp.predict_proba([x])\n",
    "    row = []\n",
    "    row.append(tup[1])\n",
    "\n",
    "    for e in res[0]:\n",
    "        row.append(e)\n",
    "\n",
    "    #result_file_obj.writerow(row)\n",
    "\n",
    "#result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068369ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ad16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d70973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f24e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
